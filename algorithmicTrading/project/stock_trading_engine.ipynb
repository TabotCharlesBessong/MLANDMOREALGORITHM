{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.regression.rolling import RollingOLS\n",
    "import pandas_datareader as web\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "import pandas_ta\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Downloading dataset and organising data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "sp500.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500.to_csv('../DATA/SP500.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500['Symbol'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500['Symbol'] = sp500['Symbol'].str.replace('.','-')\n",
    "symbols_list = sp500['Symbol'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please do not run the cell below again it is consumming me plenty of data `Only for the author`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = '2023-09-27'\n",
    "\n",
    "start_date = pd.to_datetime(end_date) - pd.DateOffset(365*8)\n",
    "\n",
    "data = yf.download(tickers=symbols_list,start=start_date,end=end_date).stack()\n",
    "data.index.names = ['date','ticker']\n",
    "data.columns = data.columns.str.lower()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please do not run the cell above again it is consumming me plenty of data `Only for the author`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../DATA/dataF.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Garmman Klass Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['german_klass_vol'] = ((np.log(data['high'])-np.log(data['low']))**2)/2-(2*np.log(2)-1)*((np.log(data['adj close'])-np.log(data['open']))**2)\n",
    "\n",
    "data['rsi'] = data.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.rsi(close=x,length=20))\n",
    "\n",
    "data['bb_low'] = data.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.bbands(close=np.log1p(x),length=20).iloc[:,0])\n",
    "\n",
    "data['bb_mid'] = data.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.bbands(close=np.log1p(x), length=20).iloc[:,1])\n",
    "                                                          \n",
    "data['bb_high'] = data.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.bbands(close=np.log1p(x), length=20).iloc[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.xs('AAPL',level=1)['rsi'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.xs('AAPL',level=1)['bb_low'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.xs('AAPL',level=1)['bb_mid'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.xs('AAPL',level=1)['bb_high'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_atr(stock_data):\n",
    "  atr = pandas_ta.atr(high=stock_data['high'],low=stock_data['low'],close=stock_data['close'],length=14)\n",
    "  \n",
    "  return atr.sub(atr.mean()).div(atr.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['atr'] = data.groupby(level=1,group_keys=False).apply(compute_atr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_macd(close):\n",
    "  macd = pandas_ta.macd(close=close,length=20).iloc[:,0]\n",
    "  return macd.sub(macd.mean()).div(macd.std())\n",
    "\n",
    "data['macd'] = data.groupby(level=1,group_keys=False)['adj close'].apply(compute_macd)\n",
    "\n",
    "data['dollar_volume'] = (data['adj close'] * data['volume']) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[900000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Aggregate to monthly level and filter top 150 most liquid stocks for each month.\n",
    "\n",
    "* To reduce training time and experiment with features and strategies, we convert the business-daily data to month-end frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cols = [c for c in data.columns.unique(0) if c not in ['dollar_volume','volume','open','high','low','close']]\n",
    "\n",
    "data = (pd.concat([data.unstack('ticker')['dollar_volume'].resample('M').mean().stack('ticker').to_frame('dollar_volume'),data.unstack()[last_cols].resample('M').last().stack('ticker')],axis=1)).dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate 5-year rolling average of dollar volume for each stocks before filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dollar_volume'] = (data.loc[:,'dollar_volume'].unstack('ticker').rolling(5*12,min_periods=12).mean().stack())\n",
    "\n",
    "data['dollar_vol_rank'] = (data.groupby('date')['dollar_volume'].rank(ascending=False))\n",
    "\n",
    "data = data[data['dollar_vol_rank']<150].drop(['dollar_volume', 'dollar_vol_rank'], axis=1)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Monthly Returns for different time horizons as features.\n",
    "\n",
    "* To capture time series dynamics that reflect, for example, momentum patterns, we compute historical returns using the method .pct_change(lag), that is, returns over various monthly periods as identified by lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(df):\n",
    "\n",
    "    outlier_cutoff = 0.005\n",
    "\n",
    "    lags = [1, 2, 3, 6, 9, 12]\n",
    "\n",
    "    for lag in lags:\n",
    "\n",
    "        df[f'return_{lag}m'] = (df['adj close']\n",
    "                              .pct_change(lag)\n",
    "                              .pipe(lambda x: x.clip(lower=x.quantile(outlier_cutoff),\n",
    "                                                     upper=x.quantile(1-outlier_cutoff)))\n",
    "                              .add(1)\n",
    "                              .pow(1/lag)\n",
    "                              .sub(1))\n",
    "    return df\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.groupby(level=1, group_keys=False).apply(calculate_returns).dropna\n",
    "data = data.groupby(level=1,group_keys=False).apply(calculate_returns)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download Fama-French Factors and Calculate Rolling Factor Betas.\n",
    "\n",
    "* We will introduce the Fama—French data to estimate the exposure of assets to common risk factors using linear regression.\n",
    "\n",
    "* The five Fama—French factors, namely market risk, size, value, operating profitability, and investment have been shown empirically to explain asset returns and are commonly used to assess the risk/return profile of portfolios. Hence, it is natural to include past factor exposures as financial features in models.\n",
    "\n",
    "* We can access the historical factor returns using the pandas-datareader and estimate historical exposures using the RollingOLS rolling linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_data = web.DataReader('F-F_Research_Data_5_Factors_2x3','famafrench',start='2010')[0].drop('RF',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_data.index = factor_data.index.to_timestamp()\n",
    "\n",
    "factor_data = factor_data.resample('M').last(100)\n",
    "\n",
    "factor_data.index.name = 'date'\n",
    "\n",
    "factor_data = factor_data.join(data['return_1m']).sort_index()\n",
    "\n",
    "factor_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Filter out stocks with less than 10 months of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = factor_data.groupby(level=1).size()\n",
    "\n",
    "valid_stocks = observations[observations >= 10]\n",
    "\n",
    "factor_data = factor_data[factor_data.index.get_level_values('ticker').isin(valid_stocks.index)]\n",
    "\n",
    "factor_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate Rolling Factor Betas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = (factor_data.groupby(level=1,group_keys=False).apply(lambda x: RollingOLS(endog=x['return_1m'],exog=sm.add_constant(x.drop('return_1m',axis=1)),window=min(24,x.shape[0]),min_nobs=len(x.columns)+1).fit(params_only=True).params.drop('const',axis=1)))\n",
    "\n",
    "betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Join the rolling factors data to the main features dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "\n",
    "data = (data.join(betas.groupby('ticker').shift()))\n",
    "\n",
    "data.loc[:, factors] = data.groupby('ticker', group_keys=False)[factors].apply(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "data = data.drop('adj close', axis=1)\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. For each month fit a K-Means Clustering Algorithm to group similar assets based on their features.\n",
    "\n",
    "### K-Means Clustering\n",
    "* You may want to initialize predefined centroids for each cluster based on your research.\n",
    "\n",
    "* For visualization purpose of this tutorial we will initially rely on the ‘k-means++’ initialization.\n",
    "\n",
    "* Then we will pre-define our centroids for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_rsi_values = [30,45,55,70]\n",
    "\n",
    "initial_centroids = np.zeros((len(target_rsi_values),18))\n",
    "initial_centroids[:,6] = target_rsi_values\n",
    "initial_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# data = data.drop('cluster',axis=1)\n",
    "def get_clusters(df):\n",
    "  df['cluster'] = KMeans(n_clusters=4,random_state=0,init=initial_centroids).fit(df).labels_\n",
    "  \n",
    "  return df\n",
    "\n",
    "data = data.dropna().groupby('date',group_keys=False).apply(get_clusters)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(data):\n",
    "  cluster_0 = data[data['cluster']==0]\n",
    "  cluster_1 = data[data['cluster']==1]\n",
    "  cluster_2 = data[data['cluster']==2]\n",
    "  cluster_3 = data[data['cluster']==3]\n",
    "  \n",
    "  plt.scatter(cluster_0.iloc[:,0],cluster_0.iloc[:,6],color='red',label='cluster 0')\n",
    "  plt.scatter(cluster_1.iloc[:,0],cluster_1.iloc[:,6],color='green',label='cluster 1')\n",
    "  plt.scatter(cluster_2.iloc[:,0],cluster_2.iloc[:,6],color='blue',label='cluster 2')\n",
    "  plt.scatter(cluster_3.iloc[:,0],cluster_3.iloc[:,6],color='black',label='cluster 3')\n",
    "  \n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "for i in data.index.get_level_values('date').unique().tolist():\n",
    "  \n",
    "  g = data.xs(i,level=0)\n",
    "  plt.title(f'Date {i}')\n",
    "  \n",
    "  plot_clusters(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. For each month select assets based on the cluster and form a portfolio based on Efficient Frontier max sharpe ratio optimization\n",
    "\n",
    "* First we will filter only stocks corresponding to the cluster we choose based on our hypothesis.\n",
    "\n",
    "* Momentum is persistent and my idea would be that stocks clustered around RSI 70 centroid should continue to outperform in the following month - thus I would select stocks corresponding to cluster 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = data[data['cluster']==3].copy()\n",
    "filtered_df = filtered_df.reset_index(level=1)\n",
    "filtered_df.index = filtered_df.index + pd.DateOffset(1)\n",
    "filtered_df = filtered_df.reset_index().set_index(['date','ticker'])\n",
    "dates = filtered_df.index.get_level_values('date').unique().tolist()\n",
    "\n",
    "fixed_dates = {}\n",
    "\n",
    "for d in dates:\n",
    "  fixed_dates[d.strftime('%Y-%m-%d')] = filtered_df.xs(d,level=0).index.tolist()\n",
    "  \n",
    "fixed_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define portfolio optimization function\n",
    "\n",
    "* We will define a function which optimizes portfolio weights using PyPortfolioOpt package and EfficientFrontier optimizer to maximize the sharpe ratio.\n",
    "\n",
    "* To optimize the weights of a given portfolio we would need to supply last 1 year prices to the function.\n",
    "\n",
    "* Apply signle stock weight bounds constraint for diversification (minimum half of equaly weight and maximum 10% of portfolio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_weights(prices,lower_bound=0):\n",
    "  returns = expected_returns.mean_historical_return(prices=prices,frequency=252)\n",
    "  \n",
    "  cov = risk_models.sample_cov(prices=prices,frequency=252)\n",
    "  \n",
    "  ef = EfficientFrontier(expected_returns=returns,cov_matrix=cov,weight_bounds=(lower_bound,.1),solver='SCS')\n",
    "  \n",
    "  weights = ef.max_sharpe()\n",
    "  return ef.clean_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download Fresh Daily Prices Data only for short listed stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = data.index.get_level_values('ticker').unique().tolist()\n",
    "\n",
    "new_data = yf.download(tickers=stocks,start=data.index.get_level_values('date').unique()[0] - pd.DateOffset(months=12),end=data.index.get_level_values('date').unique()[-1])\n",
    "\n",
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate daily returns for each stock which could land up in our portfolio.\n",
    "\n",
    "* Then loop over each month start, select the stocks for the month and calculate their weights for the next month.\n",
    "\n",
    "* If the maximum sharpe ratio optimization fails for a given month, apply equally-weighted weights.\n",
    "\n",
    "* Calculated each day portfolio return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_dataframe = np.log(new_data['Adj Close']).diff()\n",
    "\n",
    "portfolio_df = pd.DataFrame()\n",
    "\n",
    "for start_date in fixed_dates.keys():\n",
    "  try:\n",
    "    end_date = (pd.to_datetime(start_date) + pd.offsets.MonthEnd(0)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    cols = fixed_dates[start_date]\n",
    "    \n",
    "    optimization_start_date = (pd.to_datetime(start_date) - pd.DateOffset(months=12)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    optimization_end_date = (pd.to_datetime(start_date)-pd.DateOffset(days=1)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    optimization_df = new_data[optimization_start_date:optimization_end_date]['Adj Close'][cols]\n",
    "    \n",
    "    success = False\n",
    "    \n",
    "    try:\n",
    "      weights = optimize_weights(prices=optimization_df,lower_bound=round(1/(len(optimization_df.columns)*2),3))\n",
    "      \n",
    "      weights = pd.DataFrame(weights,index=pd.Series(0))\n",
    "      \n",
    "      success = True\n",
    "    except:\n",
    "      print(f'Max Sharpe Optimization failed for {start_date}, Continuing with Equal-Weights')\n",
    "      \n",
    "    if success == False:\n",
    "      weights = pd.DataFrame([1/len(optimization_df.columns) for i in range(len(optimization_df.columns))],\n",
    "      index=optimization_df.columns.tolist(),\n",
    "      columns=pd.Series(0)).T\n",
    "      \n",
    "    temp_df = returns_dataframe[start_date:end_date]\n",
    "\n",
    "    temp_df = temp_df.stack().to_frame('return').reset_index(level=0)\\\n",
    "                .merge(weights.stack().to_frame('weight').reset_index(level=0, drop=True),\n",
    "                      left_index=True,\n",
    "                      right_index=True)\\\n",
    "                .reset_index().set_index(['Date', 'index']).unstack().stack()\n",
    "                \n",
    "    temp_df.index.names = ['date', 'ticker']\n",
    "\n",
    "    temp_df['weighted_return'] = temp_df['return']*temp_df['weight']\n",
    "\n",
    "    temp_df = temp_df.groupby(level=0)['weighted_return'].sum().to_frame('Strategy Return')\n",
    "\n",
    "    portfolio_df = pd.concat([portfolio_df, temp_df], axis=0)\n",
    "    \n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "portfolio_df = portfolio_df.drop_duplicates()\n",
    "\n",
    "portfolio_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Portfolio returns and compare to SP500 returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy = yf.download(tickers='SPY',\n",
    "                  start='2015-01-01',\n",
    "                  end=dt.date.today())\n",
    "\n",
    "spy_ret = np.log(spy[['Adj Close']]).diff().dropna().rename({'Adj Close':'SPY Buy&Hold'}, axis=1)\n",
    "\n",
    "portfolio_df = portfolio_df.merge(spy_ret,\n",
    "                                  left_index=True,\n",
    "                                  right_index=True)\n",
    "\n",
    "portfolio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "portfolio_cumulative_return = np.exp(np.log1p(portfolio_df).cumsum())-1\n",
    "\n",
    "portfolio_cumulative_return[:'2023-09-29'].plot(figsize=(16,6))\n",
    "\n",
    "plt.title('Unsupervised Learning Trading Strategy Returns Over Time')\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "\n",
    "plt.ylabel('Return')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
